# 信息论
## 信息量
在信息论中，我们认为：
- 非常可能发生的事件信息量要比较少。在极端情况下，确保能够发生的事件应该没有信息量。
- 较不可能发生的事件具有更高的信息量。
- 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。
因此，事件$x$的信息量，又称为自信息，为
$$I(x)=log\frac{1}{p(x)}$$
本质上就是事件$x$发生的概率的分母。分母越大，概率越小，信息量越大。百分百发生的事情没有任何信息。
## 信息熵
熵的目的是为了衡量[[概率论#随机变量]]的不确定性。随机变量是样本空间的随机采样得到的变量。样本空间$X$中每个事件$x$都有分布概率$p(x)$。
Entropy is a measure of the uncertainty of a random variable. Let $X$ be a discrete random vairable with alphabet $\mathcal{X}$ and probability mass function $p(x)=P\{X=x\}, x\in \mathcal{X}$. The entropy $H(X)$ of a discrete random variable $X$ is defined by
$$ H(X)=-\sum_{x\in\mathcal{X}}p(x)\log p(x)=E_p \log\frac{1}{p(X)}. $$
The log is to the base 2 and entropy is expressed in bits.

- $H(X)\geq 0$
- $H_b(X)=(log_ba)H_a(X)$, which means that entropy can be changed from one base to another by multiplying by the appropriate factor.

The joint entropy $H(X,Y)$ of a pair of discrete random variables $(X,Y)$ with a joint distribution $p(x,y)$ is defined as
$$ H(X,Y)=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)logp(x,y)=-E\log p(X,Y). $$

If $(X,Y)\sim p(x,y)$, the conditional entropy $H(Y|X)$ is defined as
$$ \begin{aligned}
H(Y|X)& =\sum_{x\in\mathcal{X}}p(x)H(Y|X=x) \\
&=-\sum_{x\in\mathcal{X}}p(x)\sum_{y\in\mathcal{Y}}p(y|x)\log p(y|x) \\
&=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(y|x) \\
&=-E\log p(Y|X).
\end{aligned} $$

Chain rule
$$ H(X,Y)=H(X)+H(Y|X) $$
$$ H(X,Y|Z)=H(X|Z)+H(Y|X,Z). $$
## KL散度
The relative entropy is a measure of the distance between two distributions.
The relative entropy or Kullback-Leibler distance between two probability mass functions $p(x)$ and $q(x)$ is defined as
$$ \begin{aligned}
D(p||q)& =\sum_{x\in\mathcal{X}}p(x)\log\frac{p(x)}{q(x)} \\
&=E_p\log\frac{p(X)}{q(X)}.
\end{aligned} $$

For joint distributions $p(x,y)$ and $q(x,y)$, the conditional relative entropy $D(p(y|x)||q(y|x))$ is defined as:
$$ D(p(y|x)||q(y|x))=E_{p(x,y)}\log \frac{p(Y|X)}{q(Y|X)}. $$

Mutual infomation is a measure of the amount of information that one random variable contains about another random variables.
Consider two random variables $X$ and $Y$ with a joint probability mass function $p(x,y)$ and marginal probability mass functions $p(x)$ and $p(y)$. The mutual information $I(X,Y)$ is the relative entropy between the joint distribution and the product distribution $p(x)p(y)$:
$$ \begin{gathered}
I\left(X;Y\right) =\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log\frac{p(x,y)}{p(x)p(y)} \\
=D(p(x,y)||p(x)p(y)) \\
=E_{p(x,y)}\log\frac{p(X,Y)}{p(X)p(Y)}. 
\end{gathered} $$
Relationship between entropy and mutual information
$$ I(X;Y)=H(X)-H(X|Y) $$
Thus, the mutual information is the reduction in the uncertainty of $X$ due to the knowledge of $Y$.
Chain rule for entropy
$$ H(X_1,X_2,\ldots,X_n)=\sum_{i=1}^nH(X_i|X_{i-1},\ldots,X_1). $$
Chain rule for information
$$ I\left(X_1,X_2,\ldots,X_n;Y\right)=\sum_{i=1}^nI\left(X_i;Y|X_{i-1},X_{i-2},\ldots,X_1\right). $$
Chain rule for relative entropy
$$ D(p(x,y)||q(x,y))=D(p(x)||q(x))+D(p(y|x)||q(y|x)). $$
Convex function definition:
A function $f(x)$ is said to be convex over an interval (a, b) if for every $x_1, x_2 \in (a, b)$ and $0\le \lambda \le 1$,  
$$ f(\lambda x_1+(1-\lambda)x_2)\leq\lambda f(x_1)+(1-\lambda)f(x_2). $$
A function $f$ is said to be strictly convex if equality holds only if $lambda = 0$ or $lambda = 1$.
Jensen's inequality
If $f$ is a convex function and $X$ is a random variable, 
$$ Ef(X)\leq f(EX). $$
Informationo inequality
Let $p(x), q(x), x\in X$, be two probability mass functions. Then
$$ D(p||q)\ge 0 $$
with equality if and only if $p(x)=q(x)$ for all $x$.
For any two random variables, X, Y,
$$ I(X;Y)\ge 0, $$
with equality if and only if $X$ and $Y$ are independent.
$$ H(X|Y)\le H(X) $$
with equality if and only if $X$ and $Y$ are independent.
Independence bound on entropy
$$ H(X_1, X_2, \ldots, X_n)\le\sum_{i=1}^nH(X_i)$$
with equality if and only if $X_1, X_2, \ldots, X_n$ are independent.
Data processing inequality
If $X\rightarrow Y\rightarrow Z$, then $ I(X;Y)\ge I(X;Z) $
A function $T(X)$ is said to be a sufficient statistic relative to the family $\{f_\theta(x)\}$ if $X$ is independent of $\theta$ given $T(X)$ for any distribution on $\theta$[i.e., $\theta\to T(X)\to X$ forms a Markov chain].
This is the same as the condition for equality in the data-processing
inequality,
$$I\left(\theta;X\right)=I\left(\theta;T\left(X\right)\right)$$
for all distributions on $\theta.$ Hence sufficient statistics preserve mutual information and conversely.
A statistic $T(X)$ is a minimal sufficient statistic relative to $\{f_{\theta}(x)\}$ if it is a function of every other sufficitient statistic $U$. Interpreting this in terms of the data-processing inequality, this implies that  
$$ \theta \to T(X) \to U(X) \to X $$
Hence, a minimal sufficient statistic maximally compresses the information about $\theta$ in the sample. Other sufficient statistics may contain addtional irrelevant information.

>**为什么既有 KL 散度又有交叉熵**
>在信息论中，熵的意义是对 P 事件的随机变量编码所需的最小字节数，KL 散度的意义是“额外所需的编码长度”如果我们使用 Q 的编码来表示 P，交叉熵指的是当你使用 Q 作为密码来表示 P 时所需要的 “平均的编码长度”。但是在机器学习评价两个分布之间的差异时，由于分布 P 会是给定的，所以此时 KL 散度和交叉熵的作用其实是一样的，而且因为交叉熵少算一项，更加简单，所以选择交叉熵会更好。