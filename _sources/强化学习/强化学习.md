# 强化学习

## 核心概念

在马尔可夫决策过程环境中，智能体根据状态选择动作，执行动作得到奖励和下一个状态。策略是智能体根据状态选择动作的函数。期望回报是智能体根据策略与环境交互得到的所有轨迹的回报的期望值。强化学习通过优化策略使得期望回报最大化的方法。

主要问题：
1. 如何选择动作？
2. 如何调整策略？

选择动作的两种思路：
1. 定义动作价值函数，根据价值选择动作，通过奖励值调整动作价值
2. 定义策略函数，根据策略选择动作，通过奖励值调整策略函数参数

## 环境模型

### 马尔可夫性
满足以下条件的过程我们称为具有马尔可夫性，即下一个状态仅与当前状态有关，而与历史状态无关。
$$ P(S_{t+1}|S_t)=P{S_{t+1}|S_1,...,S_t} $$

### 马尔可夫决策过程模型

马尔可夫过程是一个具备马尔可夫性质的离散随机过程。马尔可夫性质即下一状态只与当前状态有关，与之前的状态无关，也被称为时间同质性。马尔可夫过程由状态集和状态转移矩阵构成。

智能体可以通过马尔可夫链的状态转移函数来实现与环境的交互，但是马尔可夫过程并不能让环境提供奖励反馈给智能体。因此，马尔可夫奖励过程加入了奖励函数和奖励折扣因子。

折扣化回报：奖励值的加权求和。解决的问题是使回报值收敛，距离更近的时间步比相对较远的时间步产生更大的影响。

强化学习以马尔可夫决策过程作为环境模型。

马尔可夫决策过程是一个五元组 $<S, A, \boldsymbol{P}, R, \gamma>$,分别代表：

$S = \{s_1, s_2, ..., s_n\}$ - 状态空间，包含模型所有可能的状态值

$A = \{a_1, a_2, ..., a_m\}$ - 动作空间，包含模型所有可能的动作值

$\boldsymbol{P}(s,a) = p(S_{t+1}|S_t=s, A_t=a)$ - 状态转移概率函数，输出是转移到其他状态的概率向量

${P}(s'|s,a) = p(S_{t+1}=s'|S_t=s, A_t=a)$ - 表示状态s执行动作a后转移到状态s'的概率值

$R(s) = \mathbb{E}(R_{t+1}|S_t=s)$ - 奖励函数，某个状态的奖励指转移到该状态时可以获得奖励的期望。

$R(s,a)$ - 奖励函数，表示状态s执行动作a后获得的奖励。

$\gamma$ - 折扣因子，常量

## 问题建模

### 基于策略

强化学习的目的是找到一个策略使智能体通过策略与环境交互的期望回报最大化。

策略函数用$\theta$来定义：
$$\pi_\theta(a|s)=p(A_t=a|S_t=s)$$

轨迹$ \tau=(S_1,A_1,S_2,A_2,...,S_T,A_T) $
根据策略生成轨迹的概率：
$$p_\theta(\tau)=p(S_1)\prod_{t=1}^{T}\pi_\theta(A_t|S_t)P(S_{t+1}|S_t,A_t)$$

由概率的乘法公式，$p(S_1S_2...S_T)=p(S_1)p(S_2|S_1)p(S_3|S_2S_1)...p(S_T|S_{t-1}...S_1)$。由马尔可夫性质，公式简化为$p(S_1S_2...S_T)=p(S_1)p(S_2|S_1)p(S_3|S_2)...p(S_T|S_{t-1})$。由马尔可夫决策模型可得$p(S_{t+1}|S_t) = \pi_\theta(A_t|S_t)P(S_{t+1}|S_t, A_t)$。
为什么这里会用到$t+1$? 因为轨迹中包含了$A_T$，想要得到$A_T$就要计算$p(S_{t+1}|S_t)$。

策略的期望回报：
$$J(\theta)=\int_\tau p_\theta(\tau)R(\tau)=\mathbb{E}_{\tau\sim p_\theta(\tau)}[R(\tau)]$$

直接通过优化策略得到最优策略函数
$$\theta^*=\arg\max_\theta J(\theta)$$


### 基于价值

基于价值的方法中，使用贪心策略选择动作，即选择当前状态下的期望最大的动作。价值的定义与求解是这类方法的核心。

价值函数定义为从状态出发遵循策略能获得的期望回报：
$$\begin{aligned}
V^{\pi}(s)&=\mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\ldots|S_t=s] \\
&=\mathbb{E}[R_{t}+\gamma(R_{t+1}+\gamma R_{t+2}+\ldots)|S_{t}=s] \\
&=\mathbb{E}[R_t+\gamma V^{\pi}(S_{t+1})|S_t=s] \\
&=\sum_{a\in A}\pi(a|s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^\pi(s^{\prime})\right)
\end{aligned}$$

动作价值函数定义为从状态和动作出发遵循策略能获得的期望回报：
$$\begin{aligned}
Q^{\pi}(s,a)& =\mathbb{E}_\pi[R_t+\gamma Q^\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]  \\
&=R(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)\sum_{a^{\prime}\in A}\pi(a^{\prime}|s^{\prime})Q^{\pi}(s^{\prime},a^{\prime})
\end{aligned}$$

贝尔曼方程：
$$\begin{aligned}
V^{\pi}(s)& =\mathbb{E}_\pi[R_t+\gamma V^\pi(S_{t+1})|S_t=s]  \\
&=\sum_{a\in A}\pi(a|s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^\pi(s^{\prime})\right) \\
Q^{\pi}(s,a)& =\mathbb{E}_\pi[R_t+\gamma Q^\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]  \\
&=R(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)\sum_{a^{\prime}\in A}\pi(a^{\prime}|s^{\prime})Q^{\pi}(s^{\prime},a^{\prime})
\end{aligned}$$


有了Q值对策略进行评估之后，我们只需要找到一种能提升Q值的方法就能提升策略的效果。从另一个角度来看，价值函数和动作价值函数也是期望回报的一种形式，使价值函数最大化与期望回报最大化本质上是一致的。
贝尔曼方程的意义是，状态的价值函数等于即时奖励加上未来可能的奖励的期望，也就是所有可能转移到的下一个状态的价值的加权和。

## 优化方法
基于策略的模型中，优化目标是找到最优的策略函数参数。首先使用神经网络表示策略函数，将策略函数参数化为$\pi(\theta)$，参数化后的策略函数可以直接使用梯度上升方法，对期望回报求关于参数的梯度，使期望回报的值最大化。

基于价值的模型中，优化目标是找到价值函数与动作价值函数的值。通过矩阵运算，可以直接得到解析解。解析解的计算复杂度是$O(n^3)$，n是状态的数量，因此只有状态规模很小时才适用。
常用动态规划、蒙特卡洛、时间差分这三类方法估计价值函数。提升Q值的方法则通常使用时间差分方法，使用$\epsilon$-贪心策略选择目标Q值，确保目标值不小于当前值。
### 代数求解


### 动态规划
动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

需要事先知道环境的转移函数和奖励函数。

#### 策略迭代
1. 策略评估

用上一轮的状态价值函数计算当前的状态价值函数。
$$ V^{k+1}(s)=\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s'\in S}P(s'|s,a)V^k(s')\right) $$ 
当状态价值函数趋近于不变时，结束迭代。

2. 策略提升
    
策略提升定理：$$\exists \pi', \forall s\in S, Q^\pi(s, \pi'(s))\geq V^\pi(s) \Rightarrow V^{\pi'}(s)\geq V^\pi(s)$$
使用贪心策略：
$$ \pi'(s)=\arg\max_aQ^\pi(s,a)=\arg\max_a\{r(s,a)+\gamma\sum_{s'}P(s'|s,a)V^\pi(s')\} $$
构造的贪心策略$\pi'$满足策略提升定理的条件，所以策略$pi'$能够比策略$pi$更好或者至少与其一样好。这个根据贪心法选取动作从而得到新的策略的过程称为策略提升。当策略提升之后得到的策略$\pi'$和之前的策略$\pi$一样时，说明策略迭代达到了收敛，此时$\pi$和$\pi'$就是最优策略。

#### 价值迭代

$$ V^{k+1}(s)=\max_{a\in\mathcal{A}}\{r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^{k}(s')\} $$

当价值不变时停止迭代，恢复策略：
$$ \pi(s)=\arg\max_a\{r(s,a)+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)V^{k+1}(s^{\prime})\} $$

### 蒙特卡洛
通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。

$$ V^{\pi}(s) \approx \frac1{N} \sum_{i=0}^nG_t^{(i)} $$

### 时间差分

$$ V(s_t)\leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)] $$

