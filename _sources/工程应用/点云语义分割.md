# 点云语义分割
## 数据结构
## Octree
[Octree - Open3D 0.19.0 documentation](https://www.open3d.org/docs/release/tutorial/geometry/octree.html)
八叉树是将三维数据进行划分存储的一种方式。将一个体素格进行分割，能够分成八个子格子，因此八叉树的八个子节点即父节点分成的子格子。若格子内无点，则不需要继续分割，否则可以继续分割，直到每个叶子节点都只有一个点。
## KDTree
[KDTree - Open3D 0.19.0 documentation](https://www.open3d.org/docs/release/tutorial/geometry/kdtree.html)
KD树可以用于最近邻搜索。输入K维数据，按不同维度依方差顺序找均值划分二叉树，即KD树。最近邻搜索时，依坐标值找到叶子节点，若距离小于均值，则叶子节点是最近邻，否则再查找下一分支。

## 数据处理
## Farthest Point Sampling
最远点采样是一种采样方法，目的是使采样尽可能的离散均匀。算法流程是：
1. 随机选取一个点加入采样集合
2. 寻找剩余点中与采样集合的距离最远的点，加入采样集合
3. 计算点与采样集合中所有点的的距离，最短距离即点与采样集合的距离
4. 重复以上步骤直到采样到足够点
最远点采样需要三重for循环（采样点、剩余点、点与已采样点距离），因此计算量大，速度慢，优点是采样均匀，能够保持三维形状特征。
## 算法模型
## Point Transformer
Point Transformer使用Transformer架构进行点云语义分割，提出了向量注意力的概念。缩放点积注意力使用点积计算查询和键之间的关系，而向量注意力用减法衡量二者之间的关系。

$$\begin{align}
softmax(\mathrm{MLP}(Q-K)+\delta)(V+\delta)\\
\delta = \mathrm{MLP}(p_i-p_j)
\end{align}$$
这样做的好处是与价值向量相乘的不再是一个点积标量而是一个向量，表达能力更强。
## Point Transformer V2
PTv1的缺点是，随着数据规模的上升，参数规模也会变得非常大。因此PTv2提出了一个折中的办法，就是用分组的向量注意力机制，将一个向量分成几块，每块共享一个参数。
另一个改进点是，PTv1的时候，用的是kNN查询近邻向量作为一个子集进行注意力训练，大量的时间花在了kNN查询中。因此，PTv2划分了网格，直接将网格内的点作为一个子集训练注意力。
## Point Transformer V3
[GitHub - Pointcept/Pointcept: Pointcept: a codebase for point cloud perception research. Latest works: Sonata (CVPR'25 Highlight), PTv3 (CVPR'24 Oral), PPT (CVPR'24), MSC (CVPR'23)](https://github.com/Pointcept/Pointcept)
使用点云序列化的方式加速了模型的推理过程，利用固定的模式将三维点云映射到一维，每个点云都有一个下标，且相邻下标的点在几何上距离相近，这样就省去了在三维空间查找的时间。
## 加速优化
## Flash Attention
[Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file)
利用GPU的SRAM、HBM和CPUDRAM的处理带宽和容量的差异，将QKV向量分块后使用SRAM计算sm(QK)V的值。关键是使用safe softmax进行分块后的注意力值计算公式。
[torch.nn.functional.scaled_dot_product_attention — PyTorch 2.2 documentation](https://docs.pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html)
Flash Attention目前已经集成到pytorch中。