# 模型部署
## ONNX
[onnx/tutorials: Tutorials for creating and using ONNX models](https://github.com/onnx/tutorials)
## 目的
**ONNX**（英语：Open Neural Network Exchange）是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的人工智能框架（如Pytorch、MXNet）可以采用相同格式存储模型数据并交互。
## 方法
[将 PyTorch 训练模型转换为 ONNX | Microsoft Learn](https://learn.microsoft.com/zh-cn/windows/ai/windows-ml/tutorials/pytorch-convert-model)
[Export a PyTorch model to ONNX — PyTorch Tutorials 2.5.0+cu124 documentation](https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html)
## TensorRT
[NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/index.html)
## 目的
**量化**在数字信号处理领域是指将信号的连续取值（或者大量可能的离散取值）近似为有限多个（或较少的）离散值的过程。模型量化是指将神经网络的浮点算法转换为定点。
-  减小模型大小：如 int8 量化可减少 75% 的模型大小，int8 量化模型大小一般为 32 位浮点模型大小的 1/4
	- 减少存储空间：在端侧存储空间不足时更具备意义。
	- 减少内存占用：更小的模型当然就意味着不需要更多的内存空间。
	- 减少设备功耗：内存耗用少了推理速度快了自然减少了设备功耗；
- 加快推理速度，访问一次 32 位浮点型可以访问四次 int8 整型，整型运算比浮点型运算更快；CPU 用 int8 计算的速度更快
- 某些硬件加速器如 DSP/NPU 只支持 int8。比如有些微处理器属于 8 位的，低功耗运行浮点运算速度慢，需要进行 8bit 量化。
## 方法
NVIDIA® TensorRT™ 是一个用于优化训练好的深度学习模型的SDK，旨在实现高性能推理。TensorRT 包含了一个深度学习推理优化器和一个运行时环境以供执行。当你在一个自己选择的框架中训练好你的深度学习模型之后，TensorRT 能够让你以更高的吞吐量和更低的延迟来运行该模型。
[Quick Start Guide :: NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ex-deploy-onnx)
## Triton
[Triton Inference Server — NVIDIA Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)
## 目的
Triton 推理服务器可助力团队在任意基于 GPU 或 CPU 的基础设施上部署、运行和扩展任意框架中经过训练的 AI 模型，进而精简 AI 推理。