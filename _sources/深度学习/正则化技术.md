# 正则化
机器学习的一个核心问题是设计不仅在训练数据上表现好，而且能在新输入上泛化好的算法。正则化是对学习算法的修改，旨在减少泛化误差而不是训练误差。
## 权重衰减
权重衰减即L2正则化，在loss上加入权重的L2正则。逻辑是利用权重与0的距离作为函数复杂度的衡量值，我们希望不复杂的模型，这样泛化性更高，因此将复杂度作为惩罚项加入Loss中，权重大时Loss值也变大，模型就会更快改变。
$$J'(\theta)=J(\theta)+\frac{\alpha}{2}\theta^T\theta$$
## 数据增强
使用更多数据训练可以增加泛化性，在实践中，我们用拥有的数据量是很有限的，解决这个问题的一种方法是创建假数据并添加到训练集中。
在神经网络的输入层中注入噪声也可以看作数据增强的一种方式。
## 提前终止
提前终止训练迭代，返回使验证集误差最低的参数设置。
## 暂退法
暂退法的思想是给神经网络的层注入噪声，使学习映射更平滑，减少过拟合。
$$h'=\begin{cases}0&p\\\frac{h}{1-p}&1-p\end{cases}$$
此时$E[h']=h$，期望值保持不变，引入噪声是无偏的。
## 批量正则化
[BatchNorm1d — PyTorch 2.7 documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)
$$y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon}}*\gamma + \beta$$
批量正则化用在线性层后，激活函数前。