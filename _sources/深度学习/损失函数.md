# 损失函数
[torch.nn — PyTorch 2.5 documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)
数理统计的基本假设是随机采样得到的数据满足独立同分布。样本$X$和标签$Y$服从真实概率分布$p_{real}(X,Y)$，我们无法得到真实概率分布的情况，但是可以使用训练数据的经验分布$p(X,Y)$作为替代，即用频率替代概率。
模型$f(\theta)$根据$X$得到的估计值$\hat{Y}=f(X|\theta)$服从$\hat p(X,\hat{Y})$。性能度量需要衡量两个分布之间的距离，用来衡量距离的函数称为损失函数。如何得到最优参数使得两个分布之间的损失最小？最大似然法可以用来估计参数。对$\theta$的最大似然被定义为$$\begin{align}
\theta_{ML} &= \arg \max_{\theta}\hat p(X,Y|\theta)\\
&= \arg\max_{\theta}\Pi \hat p(x,y|\theta)\\
&= \arg\max_{\theta}\Sigma \log \hat p(x,y|\theta)\\
&= \arg\max_{\theta}E_p \log \hat p(x,y|\theta)\\
\end{align}$$当任务为拟合任务，标签是单个值时，可以直接使用均方误差作为损失函数。当任务为分类任务，标签是一组用来表示分布的向量时，可以用交叉熵，等价于KL散度，来衡量两个分布之间的距离。
## L1Loss
$$L1Loss(y,\hat y)=|y-\hat y|$$
## MSELoss(L2Loss)
$$MSELoss(y,\hat y) =\left(y-\hat y\right)^2$$
## CrossEntropyLoss
$$CrossEntropyLoss(y, \hat y)=-\Sigma_i y_i\log\hat y_i$$
在分类任务中，若类别是$C$，使用One-hot处理标签$Y$表示类别分布，用Softmax处理输出层结果$O$得到$\hat Y$表示预测类的概率分布，用交叉熵算两个分布的损失。
> 💡 **为什么KL散度和交叉熵是等价的？**
> 真实标签的分布$p$与预测标签的分布$\hat p$的KL散度可以写为$$\begin{align}
D(p||\hat p) &= E_p\log \frac{p}{\hat p}\\
&=E_p\log p-E_p\log \hat p
\end{align}$$其中第一项$E_p\log p$是固定值，因为真实标签的分布不会随模型参数$\theta$变化而变化，因此去掉第一项后的KL散度就是交叉熵$$CrossEntropyLoss(p,\hat p)=-E_p\log \hat p$$最小化交叉熵与最大似然估计也是等价的。
