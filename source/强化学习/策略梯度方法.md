# 策略梯度方法

轨迹定义：
$$\tau = (s_1, a_1, s_2, a_2, ..., s_T, a_T)$$
轨迹分布定义：
$$p_\theta(\tau)=p(s_1)\prod_{t=1}^{T}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$$
回报定义：
$$J(\theta) = E_{\tau\sim p_\theta(\tau)}[r(\tau)] $$

## REINFORCE

对参数$\theta$求导，得到策略梯度：
$$\begin{align}
   \nabla_\theta J(\theta)  &= \nabla_\theta E_{\tau\sim p_\theta(\tau)}[r(\tau)] \\
     &= \nabla_\theta \int_\tau p_\theta(\tau)r(\tau)d\tau \\
     &= \int_\tau \nabla_\theta p_\theta(\tau)r(\tau)d\tau \\
     &= \int_\tau p_\theta(\tau)\nabla_\theta \log p_\theta(\tau)r(\tau)d\tau \\
     &= E_{\tau\sim p_\theta(\tau)}\left[\nabla_\theta \log p_\theta(\tau)r(\tau)\right] \\
     &= E_{\tau\sim p_\theta(\tau)}\left[\nabla_\theta\log\left(p(s_1)\prod_{t=1}^{T}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)\right)\left(\sum_{t=1}^Tr(s_t, a_t)\right)\right] \\
     &= E_{\tau\sim p_\theta(\tau)}\left[\nabla_\theta\left(\log p(s_1)+\sum_{t=1}^{T}\left(\log\pi_\theta(a_t|s_t)+\log p(s_{t+1}|s_t, a_t)\right)\right)\left(\sum_{t=1}^Tr(s_t, a_t)\right)\right] \\
     &= E_{\tau\sim p_\theta(\tau)}\left[\left(\nabla_\theta \log p(s_1)+\sum_{t=1}^{T}\nabla_\theta \log\pi_\theta(a_t|s_t)+\sum_{t=1}^T\nabla_\theta \log p(s_{t+1}|s_t, a_t)\right)\left(\sum_{t=1}^Tr(s_t, a_t)\right)\right] \\
     &=E_{\tau\sim p_\theta(\tau)}\left[\left(\sum_{t=1}^{T}\nabla_\theta \log \pi_\theta(a_t|s_t)\right)\left(\sum_{t=1}^{T}r(s_t, a_t)\right)\right]
 \end{align}$$
第2步采用了期望的积分形式。第3步将梯度放入积分内，因为累和的求导与求导的累和相同。第4步根据$p_\theta\nabla_\theta \log p_\theta(\tau) = p_\theta (\tau)\frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)}=\nabla_\theta p_\theta (\tau)$ 进行变换，因此第5步可以变回期望形式。第6步展开$p_\theta(\tau)$，第7步用$log$函数特点将乘法转换成加法，第8步将求导代入到加法中的每一项，因此第9步便可以约去与$\theta$无关的求导项，因为其结果为0。

使用蒙特卡洛方法可以估计期望值：
$$ \nabla_\theta \hat J(\theta) \approx \frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \right)\left(\sum_{t=1}^T r(s_{i,t}, a_{i,t})\right) $$

蒙特卡洛估计是无偏估计，但是估计值与真实值之间的方差较大，有效性低。我们希望得到有效性更高的估计量。

由公式5计算蒙特卡洛估计值的方差为
$$ \hat V=E_\tau\left[\left(\nabla_\theta \log p_\theta(\tau)r(\tau)-\nabla_\theta J(\theta)\right)^2\right]$$
由于$E_\tau\left[\nabla_\theta log p_\theta(\tau)b \right]=\int_\tau p_\theta(\tau) \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)}b d\tau=b\nabla_\theta\int_\tau p_\theta(\tau) d_\tau=b\nabla_\theta1=0$，可以在保持期望值不变的同时引入基线，将$\nabla_\theta J(\theta)$修改为：
$$\nabla_\theta \hat J(\theta)=E_\tau[\nabla_\theta \log p_\theta(\tau)(r(\tau)-b)]$$
再次计算方差为：
$$ \bar V=E_\tau\left[\left(\nabla_\theta \log p_\theta(\tau)(r(\tau)-b) - \nabla_\theta J(\theta)\right)^2\right] $$
比较两个方差估计量的值：
$$ \hat V - \bar V = E_\tau[(\nabla_\theta \log p_\theta(\tau))^2b(2r(\tau)-b)]$$
$$ b^* =\frac {E_\tau[(\nabla_\theta \log p_\theta(\tau))^2r(\tau)]}{E_\tau[(\nabla_\theta \log p_\theta(\tau))^2]}$$
当$b=b^*$时，上式取得最大值，$\hat V - \bar V > 0$，也就是说$\bar V$要比$\hat V$小，引入基线后使方差降低了。

引入基线后的策略梯度表达式为：
$$ \nabla_\theta J(\theta) \approx \frac{1}{N}\sum_{i=1}^N  \left[\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\right) \left(\sum_{t=1}^T r(s_{i,t}, a_{i,t})-b\right)\right] $$
$$b=\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T r(s_{i,t}, a_{i,t})$$
实践中基线一般不采用最大值，而是使用奖励均值的方法。

## Actor-Critic

因为因果关系，$t$时间做的决策并不会影响$t$时间前的回报，因此也可以把期望回报写成下面的形式：

$$
\begin{align}
\nabla_\theta J(\theta) = E_{\tau\sim p_\theta(\tau)}\left[\sum_{t=1}^T \left(\nabla_\theta \log \pi_\theta(a_t|s_t) \sum_{t'=t}^T r(s_{t'}, a_{t'})\right) \right]
\end{align}
$$

如果$T=2$，这种写法比前一种写法少了$\nabla_\theta \log \pi_\theta(a_2|s_2)r(s_1,a_1)$这一项，就是不计算时间2时的决策带给时间1的收益。

我们可以将奖励定义为动作价值函数：

$$ Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})=\sum_{t^{\prime}=t}^{T}E_{\pi_{\theta}}\left[r(\mathbf{s}_{t^{\prime}},\mathbf{a}_{t^{\prime}})|\mathbf{s}_{t},\mathbf{a}_{t}\right] $$

$$ V^{\pi}(\mathbf{s}_{t})=E_{\mathbf{a}_{t}\sim\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})}[Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})] $$

$$ A^\pi(\mathbf{s}_t,\mathbf{a}_t)=Q^\pi(\mathbf{s}_t,\mathbf{a}_t)-V^\pi(\mathbf{s}_t) $$

$$ \nabla_\theta J(\theta)\approx\frac1N\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\log\pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})A^\pi(\mathbf{s}_{i,t},\mathbf{a}_{i,t}) $$

## TRPO

$$ \begin{align}
J(\theta')-J(\theta)& =J(\theta^{\prime})-E_{\mathbf{s}_{0}\sim p(\mathbf{s}_{0})}\left[V^{\pi_{\theta}}(\mathbf{s}_{0})\right] \\
&=J(\theta^{\prime})-E_{\tau\sim p_{\theta^{\prime}}(\tau)}\left[V^{\pi_{\theta}}(\mathbf{s}_{0})\right] \\
&=J(\theta')-E_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta}}(\mathbf{s}_{t})-\sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta}}(\mathbf{s}_{t})\right] \\
&=J(\theta')+E_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^\infty\gamma^t(\gamma V^{\pi_\theta}(\mathbf{s}_{t+1})-V^{\pi_\theta}(\mathbf{s}_t))\right] \\
&=E_{\tau\sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty}\gamma^{t}r(\mathbf{s}_{t},\mathbf{a}_{t})\right]+E_{\tau\sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty}\gamma^{t}(\gamma V^{\pi_{\theta}}(\mathbf{s}_{t+1})-V^{\pi_{\theta}}(\mathbf{s}_{t}))\right] \\
&=E_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^{\infty}\gamma^{t}(r(\mathbf{s}_{t},\mathbf{a}_{t})+\gamma V^{\pi_{\theta}}(\mathbf{s}_{t+1})-V^{\pi_{\theta}}(\mathbf{s}_{t}))\right] \\
&=E_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^{\infty}\gamma^{t}A^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})\right] \\
& =\sum_{t}E_{\mathbf{s}_{t}\sim p_{\theta^{\prime}}(\mathbf{s}_{t})}\left[E_{\mathbf{a}_{t}\sim\pi_{\theta^{\prime}}(\mathbf{a}_{t}|\mathbf{s}_{t})}\left[\gamma^{t}A^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})\right]\right] \\
&=\sum_{t}E_{\mathbf{s}_{t}\sim p_{\theta^{\prime}}(\mathbf{s}_{t})}\left[E_{\mathbf{a}_{t}\sim\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})}\left[\frac{\pi_{\theta^{\prime}}(\mathbf{a}_{t}|\mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})}\gamma^{t}A^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})\right]\right]
\end{align} $$

公式11中使用$s_0\sim p(s_0)$，表示期望回报与$\theta$无关，因此可以从公式11到公式12。公式19使用了重要性采样变换。  
现在我们想将$s_t\sim p_{\theta'(s_t)}$修改为$s_t\sim p_{\theta(s_t)}$，这样$J(\theta')-J(\theta)\approx\bar A(\theta')$。

$$
\begin{align}
\theta'\leftarrow \arg \max_{\theta'}\sum_{t}E_{\mathbf{s}_{t}\sim p_{\theta}(\mathbf{s}_{t})}\left[E_{\mathbf{a}_{t}\sim\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})}\left[\frac{\pi_{\theta^{\prime}}(\mathbf{a}_{t}|\mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})}\gamma^{t}A^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})\right]\right]
\\
	\mathrm{such\ that}\ D_{KL}(\pi_{\theta^{\prime}}(\mathbf{a}_t\vert s_t)||\pi_{\theta}(\mathbf{a}_t\vert s_t))\leq \epsilon
\end{align}
$$

使用拉格朗日法将函数加入到目标函数中。

$$\mathcal L(\theta^{\prime},\lambda)=\sum_{t}E_{\mathbf{s}_{t}\sim p_{\theta}(\mathbf{s}_{t})}\left[E_{\mathbf{a}_{t}\sim\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})}\left[\frac{\pi_{\theta^{\prime}}(\mathbf{a}_{t}|\mathbf{s}_{t})}{\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})}\gamma^{t}A^{\pi_{\theta}}(\mathbf{s}_{t},\mathbf{a}_{t})\right]\right]
-
\lambda(D_{KL}(\pi_{\theta^{\prime}}(\mathbf{a}_t\vert s_t)||\pi_{\theta}(\mathbf{a}_t\vert s_t))-\epsilon)$$

然后执行优化算法：
1. 最大化$\mathcal L(\theta^{\prime}, \lambda)$
2. $\lambda \leftarrow \lambda + \alpha (D_{KL}(\pi_{\theta^{\prime}}(\mathbf{a}_t\vert s_t)||\pi_{\theta}(\mathbf{a}_t\vert s_t))-\epsilon)$
