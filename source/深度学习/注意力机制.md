# 注意力机制

我们可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的<Key,Value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：
$$ Attention(Query, Source) = \Sigma_{i=1}^{L_x}(Query, Key_i)*Value_i $$
其中$L_x$表示Source的长度。
注意力机制可以被抽象成以下三阶段模型：

1. 计算相似性 - 可以通过点积、cosine函数、MLP网络等计算Query与Key的相似性；
2. 归一化 - 将相似性归一化，使得相似性之和为1；
3. 输出 - 将归一化后的相似性与Value相乘，得到最终的输出。

## 点积注意力
$$Attention(Q,K,V)=softmax(QK^T)V$$
## 加性注意力
$$Attention(Q,K,V)=softmax(Linear(Q+K))V$$
## 缩放点积注意力
$$ Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{(d_k)}})V $$
因为点积得到的结果是向量各个值对应乘积的和，向量维度与值的大小正相关，因此用点积除以向量维度进行正则化。