---
sticker: ""
---
# 激活函数
[torch.nn — PyTorch 2.5 documentation#non-linear-activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
激活函数$\delta$给神经网络节点提供了非线性能力。
> 💡**为什么需要激活函数？**
> 以[[神经网络#基本模型]]中的单层神经网络模型，为例。设输入为$X$，输出为$Y$，隐藏层的输出为$H$，则该模型可以由数学公式表示为：$$\begin{align}
H&=XW^1+b^1 \\
Y&=HW^2+b^2
\end{align}
$$由于[[代数学#线性变换]]的定义，我们可以得到：$$Y=(XW^1+b^1)W^2+b^2=XW^1W^2+b^1W^1+b^2=XW+b$$此时的神经网络加入隐藏层后依旧等价于一个线性变换。
> 若在隐藏层中加入激活函数，此时的公式为：$$\begin{align}
H&=\delta(XW^1+b^1)\\
Y&=\delta(XW^1+b^1)W^2+b^2
\end{align}$$由于$\delta$不满足线性变换$T(\alpha_1+\alpha_2)=T(\alpha_1)+T(\alpha_2)$的条件，因此不能如之前一样进行等价变换，此时的神经网络就可以表示非线性了。
>[[神经网络#基本模型]]中的隐藏层是由一个线性层加一个激活函数层组成。nn.Linear只包含[[代数学#线性变换]]的能力，只有加入激活函数后才能拥有非线性变换的能力。
## Sigmoid
$$\begin{align}
Sigmoid(x)&=(1+e^{-x})^{-1}\\
Sigmoid'(x)&=Sigmoid(x)(1-Sigmoid(x))
\end{align}$$
优点：
- 平滑，易于求导
- 值域是(0,1)，适用于将预测概率作为输出的模型
缺点：
- 幂运算耗时
- 导数值域(0,0.25)，小于1，反向传播容易梯度消失
- 输出不以0为中心，可能导致模型收敛速度过慢
> 💡**为什么Sigmoid会导致梯度消失？**
> 神经网络主要的训练方法是BP算法，BP算法的基础是导数的链式法则，也就是多个导数的乘积。而sigmoid的导数最大为0.25，且大部分数值都被推向两侧饱和区域，这就导致大部分数值经过sigmoid激活函数之后，其导数都非常小，多个小于等于0.25的数值相乘，其运算结果很小。随着神经网络层数的加深，梯度后向传播到浅层网络时，基本无法引起参数的扰动，也就是没有将loss的信息传递到浅层网络，这样网络就无法训练学习了。这就是所谓的梯度消失。

> 💡**为什么输出需要以0为中心？**
> [谈谈激活函数以零为中心的问题 | 始终](https://liam.page/2018/04/17/zero-centered-active-function/)
## Tanh
$$\begin{align}
Tanh(x)&=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
Tanh'(x)&= 1-Tanh^2(x)
\end{align}$$
优点：
- 解决了上述的Sigmoid函数输出不是0均值的问题；
- Tanh函数的导数取值范围在0~1之间，优于sigmoid函数的0~0.25，一定程度上缓解了梯度消失的问题；
- Tanh函数在原点附近与y=x函数形式相近，当输入的激活值较低时，可以直接进行矩阵运算，训练相对容易；
缺点：
- 与Sigmoid函数类似，梯度消失问题仍然存在；
- 幂运算的问题仍然存在；
## ReLU
$$\begin{align}
ReLU(x)&=max\{0,x\} \\ 
ReLU'(x)&=\begin{cases}
0, x\leq0\\
1, x>1
\end{cases}
\end{align}$$
> ❗**注意**：RELU的导数在x=0处不存在，为了保证导数的连续性，在实践中将其设置为0。

优点：
- 相较于sigmoid函数以及Tanh函数来看，在输入为正时，Relu函数不存在饱和问题，即解决了gradient vanishing问题，使得深层网络可训练；
- 计算速度非常快，只需要判断输入是否大于0值；
- 收敛速度远快于sigmoid以及Tanh函数；
- Relu输出会使一部分神经元为0值，在带来网络稀疏性的同时，也减少了参数之间的关联性，一定程度上缓解了过拟合的问题；
缺点：
- Relu函数的输出也不是以0为均值的函数；
- 存在Dead Relu Problem，即某些神经元可能永远不会被激活，进而导致相应参数一直得不到更新，产生该问题主要原因包括参数初始化问题以及学习率设置过大问题；
- 当输入为正值，导数为1，在“链式反应”中，不会出现梯度消失，但梯度下降的强度则完全取决于权值的乘积，如此可能会导致梯度爆炸问题；

> 💡**Dead ReLU**
> Dead ReLU的原因是，当ReLU输入的结果为负值，ReLU的输出为0。在反向传播时，因为输出是0，ReLU的导数也将会是0，那么这个节点相关的所有参数就都不会被更新。当一个神经网络中大部分都是零输出时，梯度在反向传播时无法流动，网络变得不活跃，无法进一步学习。
> Dead ReLU是动态的问题，死去的节点也可能被重新被激活。因为其他神经元反向传播也会影响输入层的参数，有可能把Dead ReLU的神经元拉回到大于0的值域中。
## PReLU
$$PReLU(x)=max(0,x)+a*min(0,x)$$
为了解决Dead ReLU的问题，有很多ReLU激活函数的变体，如PReLU，Leaky ReLU等。本质上就是使小于0的输入一个微小的负值，使梯度可以进行反向传播。