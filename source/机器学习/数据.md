# 数据
机器学习任务不同，数据不同，但是对数据集的要求是有共性的。数据集是任务函数的采样集合。
## 数据采集

## 数据标注

## 数据划分
### 三类数据集
- 训练集 - 训练数据中用于训练模型的集合
- 验证集 - 训练数据中用于测试模型的集合，评估系统性能
- 测试集 - 用于估计训练中或训练后的泛化误差，更新超参数
### 划分训练数据
1. 数据划分为两个不相交的子集，其中一个用于训练，另一个用于测试
2. 训练数据划分为训练集和验证集，通常按80%与20%的比例划分
## 数据配置
### 配置模板
使用YAML编写配置文件，原则上应当保存数据集相关的所有配置，配置模板为
```yaml
labels：
	0：'unlabeled'
	...
color_map:
	0: [0, 0, 0]
	...
learning_labels:
	0: 'label1'
	...
split:
	train:
		- 0
		- ...
	valid:
		- 1
		- ...
	test:
		- 2
		- ...
```
说明：
1. labels存储数据集所有标签类型以及他们的原始标签
2. color_map存储数据集原始标签对应的颜色
3. learning_lables存储需要训练的标签以及训练时下标
4. split存储数据划分后的数据集包含的文件夹名称
### 配置读取
```python
import yaml
with open('dataset.yaml', 'r') as file:
    config = yaml.safe_load(file)
config['labels']
```
## 数据加载
[torch.utils.data — PyTorch 2.5 documentation](https://pytorch.org/docs/stable/data.html)
### 创建Dataset类
Custom dataset 需要实现三个函数
- \_\_init\_\_ - 初始化函数，执行一次，需要保存文件列表对象
- \_\_len\_\_ - 长度函数，返回数据集大小
- \_\_getitem\_\_ - 迭代获取对象函数，根据输入的idx返回对象
- collate_fn - 一般整理函数也会写在Dataset中，然后
```python
import os
import numpy as np
import yaml
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, dataset, splitset="train"):
        with open('semantickitti.yaml', 'r') as file:
            config = yaml.safe_load(file)
        self.labels = config['labels']
        self.learning_labels = config['learning_labels']
        self.learning_map, self.learning_map_inv = self.learning_label_map()
        self.colors = config['colors']
        self.split = config['split']
        self.splitset = splitset
        self.data_list = []
        self.label_list = []
        for folder in self.split[self.splitset]:
            self.data_list.append(sorted(os.listdir(os.path.join('data', dataset, 'sequences', folder, 'velodyne'))))
            self.label_list.append(sorted(os.listdir(os.path.join('data', dataset, 'sequences', folder, 'labels'))))

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, index):
        data = np.fromfile(self.data_list[index])
        label = np.vectorize(self.learning_map.__getitem__)(np.fromfile(self.label_list[index]))
        return data, label
        
    def learning_label_map(self):
        self.learning_map = {}
        self.learning_map_inv = {}
        for key, value in self.labels.items():
            if value in self.learning_labels.values():
                learning_key = list(self.learning_labels.keys())[list(self.learning_labels.values()).index(value)]
                self.learning_map[key] = learning_key
                self.learning_map_inv[learning_key] = key
```
### 创建Dataloader对象
#### Dataloader参数
```python
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)
```
- num_workers: 指定线程数，多线程处理加快训练速度
- collate_fn: 整理函数，该函数会在一个批次数据加载完成后执行，数据经过该函数整理后被返回
- worker_init_fn: 这里如果传入的是局部函数或者lambda函数，会影响后面使用enumerate方法进行迭代
#### 创建代码
```python
from torch.utils.data import DataLoader
from datasets.semantickitti.SemanticKittiDataset import SemanticKittiDataset

train_data = SemanticKittiDataset('data\semantickitti\sequences', 'train')
valid_data = SemanticKittiDataset('data\semantickitti\sequences', 'valid')

train_dataloader = DataLoader(dataset=train_data, batch_size=64, shuffle=True, num_workers = 4)
valid_dataloader = DataLoader(dataset=valid_data, batch_size=64, shuffle=False, num_workers = 4)
```

>**如何处理不同长度的输入数据**？
>神经网络难以处理不同长度的输入数据的原因有两点：
>1. 模型结构限制了输入的形状。比如，卷积层可以接受不同大小的输入，但是线性层不可以，其神经网络单元是固定的。
>2. 通常为了加速训练，需要批量输入训练数据，同一批数据要输入到网络，必须组织成矩阵的形式，因此需要同一个批次内的数据形状相同。
>不同的数据形状导致的问题会发生在加载数据时，因为同一批次数据形状不同导致报错。因此我们需要在collate_fn函数或者__getitem__中对数据进行填充或裁剪。一般会在__getitem__中对数据进行处理，以点云语义分割为例，数据会在__getitem__中进行划分并采样到同一个形状大小，然后在collate_fn中压入栈中，处理成模型需要的格式。
