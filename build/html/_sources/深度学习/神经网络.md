# ç¥ç»ç½‘ç»œ
## é€šç”¨è¿‘ä¼¼å®šç†
é€šç”¨è¿‘ä¼¼å®šç†ï¼ˆuniversal approximation theoremï¼‰(Hornik et al., 1989; Cybenko, 1989) è¡¨æ˜ï¼Œä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œå¦‚æœå…·æœ‰çº¿æ€§è¾“å‡ºå±‚å’Œè‡³å°‘ä¸€å±‚å…·æœ‰ä»»ä½•ä¸€ç§ â€˜â€˜æŒ¤å‹â€™â€™ æ€§è´¨çš„æ¿€æ´»å‡½æ•°ï¼ˆä¾‹å¦‚logistic sigmoidæ¿€æ´»å‡½æ•°ï¼‰çš„éšè—å±‚ï¼Œåªè¦ç»™äºˆç½‘ç»œè¶³å¤Ÿæ•°é‡çš„éšè—å•å…ƒï¼Œå®ƒå¯ä»¥ä»¥ä»»æ„çš„ç²¾åº¦æ¥è¿‘ä¼¼ä»»ä½•ä»ä¸€ä¸ªæœ‰é™ç»´ç©ºé—´åˆ°å¦ä¸€ä¸ªæœ‰é™ç»´ç©ºé—´çš„Borelå¯æµ‹å‡½æ•°ã€‚
## åŸºæœ¬æ¨¡å‹
[torch.nn â€” PyTorch 2.5 documentation](https://pytorch.org/docs/stable/nn.html#linear-layers)
ç¥ç»ç½‘ç»œå¯ä»¥è¡¨ç¤ºä»»æ„å˜æ¢ï¼Œå› æ­¤ï¼Œåªè¦æœ‰æ•°æ®å’Œæ ‡ç­¾ï¼Œå°±å¯ä»¥ä½¿ç”¨ç¥ç»ç½‘ç»œå­¦ä¹ æ•°æ®ä¸æ ‡ç­¾ä¹‹é—´çš„å…³ç³»ã€‚ç¥ç»ç½‘ç»œå±‚ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯çº¿æ€§è¾“å‡ºå±‚ï¼Œå¯ä»¥æ»¡è¶³ä»»æ„çº¿æ€§å˜æ¢ï¼›ç¬¬äºŒéƒ¨åˆ†æ˜¯[[æ¿€æ´»å‡½æ•°#æ¿€æ´»å‡½æ•°]]ï¼Œå¯ä»¥æä¾›éçº¿æ€§å˜æ¢èƒ½åŠ›ã€‚
å•ä¸ªç¥ç»å…ƒçš„ç»“æ„ä¸º
![](attachments/SingleModel.png)

> ğŸ’¡ **ä¸ºä»€ä¹ˆ$b$æ˜¯ä¸€ç»´è€Œ$W$æ˜¯å¤šç»´ï¼Ÿ**
> å‡è®¾$B\in \mathbb R^{d\times h}$ï¼Œæ ¹æ®å…¬å¼$H=XW+B$ï¼Œé‚£ä¹ˆå¯ä»¥å¾—åˆ°$$\begin{align}
h_1&=w_1\times x_1+b_1+w_2\times x_2+b_2+w_3\times x_3+b_3+w_4\times x_4+b_4\\
&=w_1\times x_1+w_2\times x_2+w_3\times x_3+w_4\times x_4+(b_1+b_2+b_3+b_4)\\
&=w_1\times x_1+w_2\times x_2+w_3\times x_3+w_4\times x_4+b\\
&=xW+b
\end{align}$$å› æ­¤$b$åªéœ€è¦ä¸€ç»´å³å¯ã€‚

ä»¥ä¸€ä¸ªå•éšè—å±‚ç¥ç»ç½‘ç»œä¸ºä¾‹

![](attachments/NeuralNetwork.png)

è®¾è¾“å…¥$X\in\mathbb R^{n\times d}$è¡¨ç¤º$n$ä¸ªå…·æœ‰$d$ç»´ç‰¹å¾çš„æ ·æœ¬ï¼Œ$H\in \mathbb R^{n\times h}$è¡¨ç¤ºéšè—å±‚çš„è¾“å‡ºï¼Œ$O\in \mathbb R^{n\times q}$è¡¨ç¤ºè¾“å‡ºå±‚çš„è¾“å‡ºã€‚è¾“å…¥ä¸€ä¸ªæ ·æœ¬$x=[x_1, x_2, x_3, x_4]$ï¼Œé¦–å…ˆç»è¿‡éšè—å±‚ï¼Œå…¬å¼ä¸º$$H=\sigma(xW^1+b^1)$$å…¶ä¸­$W^1\in \mathbb R^{d\times h}$ï¼Œ$b^1\in\mathbb R^{1\times h}$ï¼Œ$\sigma(x)$æ˜¯æ¿€æ´»å‡½æ•°ï¼Œå¦‚[[æ¿€æ´»å‡½æ•°#ReLU]]ã€‚
éšè—è¡¨ç¤ºçš„ç‰¹å¾å†è¾“å…¥è¾“å‡ºå±‚ï¼Œå…¬å¼ä¸º$$O=HW^2+b^2$$å…¶ä¸­$W^2\in \mathbb R^{h\times q}$ï¼Œ$b^2\in \mathbb R^{1\times q}$ã€‚
æ­¤æ—¶å¾—åˆ°çš„è¾“å‡º$O$çš„å–å€¼èŒƒå›´æ˜¯$\mathbb R$ã€‚åœ¨æ‹Ÿåˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°äº†æˆ‘ä»¬å¸Œæœ›çš„è¾“å‡ºï¼›åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›è¾“å‡ºå±‚çš„è¾“å‡ºå¯ä»¥è¡¨ç¤ºé¢„æµ‹å‡ºçš„ç±»åˆ«åˆ†å¸ƒæ¦‚ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç”¨softmaxå‡½æ•°å¤„ç†è¾“å‡ºç»“æœã€‚
$$\hat Y=softmax(O),å…¶ä¸­,\hat {y_i}=\frac{e^i}{\Sigma_je^j}$$
åœ¨Pytorchä¸­ï¼Œå®ç°ä¸Šè¿°æ¨¡å‹çš„ä»£ç ä¸º
```Python
nn.Sequential(
	nn.Linear(in_features, hidden_features),
	nn.ReLU(),
	nn.Linear(hidden_features, out_features),
	nn.Softmax(dim=1)
)
```
in_featuresæ˜¯è¾“å…¥å±‚çš„ç¥ç»ç½‘ç»œèŠ‚ç‚¹ä¸ªæ•°ï¼Œhidden_featuresæ˜¯éšè—å±‚çš„ç¥ç»ç½‘ç»œèŠ‚ç‚¹ä¸ªæ•°ï¼Œout_featuresæ˜¯è¾“å‡ºå±‚ç¥ç»ç½‘ç»œçš„èŠ‚ç‚¹ä¸ªæ•°ã€‚

> ğŸ’¡**ä¸ºä»€ä¹ˆPytorchå®ç°ä¸­é€šå¸¸ä¸åŠ å…¥Softmaxå±‚ï¼Ÿ**
> Pytorchåœ¨nn.CrossEntropyLossä¸­éšå¼çš„å®ç°äº†Softmaxæ“ä½œã€‚å¦‚æœä½¿ç”¨çš„Losså‡½æ•°ä¸åŒ…æ‹¬æ­¤åŠŸèƒ½ï¼ŒåŒæ—¶æ ‡ç­¾ä¹Ÿæ˜¯ä»¥One-hotæ ¼å¼ç»„ç»‡ï¼Œé‚£ä¹ˆå°±éœ€è¦è‡ªå·±å®ç°Softmaxæ“ä½œã€‚

> ğŸ’¡**ä¸ºä»€ä¹ˆä¸èƒ½ç”¨å•å±‚ç¥ç»ç½‘ç»œä½œä¸ºé€šç”¨æ¨¡å‹ï¼Œè€Œéœ€è¦è®¾è®¡ä¸åŒçš„æ¨¡å‹æ¶æ„ï¼Ÿ**
>ä¸‡èƒ½è¿‘ä¼¼å®šç†æ„å‘³ç€æ— è®ºæˆ‘ä»¬è¯•å›¾å­¦ä¹ ä»€ä¹ˆå‡½æ•°ï¼Œæˆ‘ä»¬çŸ¥é“ä¸€ä¸ªå¤§çš„ MLP ä¸€ å®šèƒ½å¤Ÿ**è¡¨ç¤º**è¿™ä¸ªå‡½æ•°ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸èƒ½ä¿è¯è®­ç»ƒç®—æ³•èƒ½å¤Ÿ**å­¦å¾—**è¿™ä¸ªå‡½æ•°ã€‚å³ä½¿ MLP èƒ½å¤Ÿè¡¨ç¤ºè¯¥å‡½æ•°ï¼Œå­¦ä¹ ä¹Ÿå¯èƒ½å› ä¸¤ä¸ªä¸åŒçš„åŸå› è€Œå¤±è´¥ã€‚é¦–å…ˆï¼Œç”¨äºè®­ç»ƒçš„ä¼˜åŒ–ç®— æ³•å¯èƒ½æ‰¾ä¸åˆ°ç”¨äºæœŸæœ›å‡½æ•°çš„å‚æ•°å€¼ã€‚å…¶æ¬¡ï¼Œè®­ç»ƒç®—æ³•å¯èƒ½ç”±äºè¿‡æ‹Ÿåˆè€Œé€‰æ‹©äº†é”™è¯¯çš„å‡½æ•°ã€‚å‰é¦ˆç½‘ç»œæä¾›äº†è¡¨ç¤ºå‡½æ•°çš„ä¸‡èƒ½ç³»ç»Ÿï¼Œåœ¨è¿™ç§æ„ä¹‰ä¸Šï¼Œç»™å®šä¸€ä¸ªå‡½æ•°ï¼Œæ·±åº¦å‰é¦ˆç½‘ç»œå­˜åœ¨ä¸€ä¸ªå‰é¦ˆç½‘ç»œèƒ½å¤Ÿè¿‘ä¼¼è¯¥å‡½æ•°ã€‚ä¸å­˜åœ¨ä¸‡èƒ½çš„è¿‡ç¨‹æ—¢èƒ½å¤ŸéªŒè¯è®­ç»ƒé›†ä¸Šçš„ç‰¹æ®Šæ ·æœ¬ï¼Œåˆèƒ½å¤Ÿé€‰æ‹©ä¸€ä¸ªå‡½æ•°æ¥æ‰©å±•åˆ°è®­ç»ƒé›†ä¸Šæ²¡æœ‰çš„ç‚¹ã€‚
>æ€»ä¹‹ï¼Œå…·æœ‰å•å±‚çš„å‰é¦ˆç½‘ç»œè¶³ä»¥è¡¨ç¤ºä»»ä½•å‡½æ•°ï¼Œä½†æ˜¯ç½‘ç»œå±‚å¯èƒ½å¤§å¾—ä¸å¯å®ç°ï¼Œ å¹¶ä¸”å¯èƒ½æ— æ³•æ­£ç¡®åœ°å­¦ä¹ å’Œæ³›åŒ–ã€‚åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œä½¿ç”¨æ›´æ·±çš„æ¨¡å‹èƒ½å¤Ÿå‡å°‘è¡¨ç¤ºæœŸæœ›å‡½æ•°æ‰€éœ€çš„å•å…ƒçš„æ•°é‡ï¼Œå¹¶ä¸”å¯ä»¥å‡å°‘æ³›åŒ–è¯¯å·®ã€‚


çº¿æ€§ä»£æ•°çš„çŸ©é˜µè¡¨ç¤ºæ–¹å¼çœ‹èµ·æ¥ä¸é‚£ä¹ˆç›´è§‚ï¼Œå°†æ‰€æœ‰å‚æ•°å±•å¼€è¡¨ç¤ºï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹æ•°æ®åœ¨ç»è¿‡æ¨¡å‹æ—¶å‘ç”Ÿäº†ä»€ä¹ˆã€‚åŸºæœ¬æ¨¡å‹çš„å‚æ•°åŒ…æ‹¬

$$
\begin{align}
\begin{bmatrix}W^1 & b^1\end{bmatrix}^T &= \begin{bmatrix}
w^1_{11} & w^1_{12} & w^1_{13} & w^1_{14} & w^1_{15} \\
w^1_{21} & w^1_{22} & w^1_{23} & w^1_{24} & w^1_{25} \\
w^1_{31} & w^1_{32} & w^1_{33} & w^1_{34} & w^1_{35} \\
w^1_{41} & w^1_{42} & w^1_{43} & w^1_{44} & w^1_{45} \\
b^1_1    & b^1_2    & b^1_3    & b^1_4    & b^1_5
\end{bmatrix} \\
\begin{bmatrix}W^2 & b^2\end{bmatrix}^T &= \begin{bmatrix}
w^2_{11} & w^2_{12} & w^2_{13} \\
w^2_{21} & w^2_{22} & w^2_{23} \\
w^2_{31} & w^2_{32} & w^2_{33} \\
w^2_{41} & w^2_{42} & w^2_{43} \\
w^2_{51} & w^2_{52} & w^2_{53} \\
b^2_1    & b^2_2    & b^2_3 
\end{bmatrix}
\end{align}
$$

è¾“å…¥ä¸€ä¸ªæ ·æœ¬$\mathbfit x=[x_1\ x_2\ x_3\ x_4]^T$ï¼Œéšè—å±‚è¾“å‡ºä¸º$H$

$$
H=\begin{bmatrix}h_1 & h_2 & h_3 & h_4 & h_5\end{bmatrix}^T=ReLU(W^1\mathbfit x+b^1)
$$
$$
H^0=\begin{cases}
h^0_1 = x_1w^1_{11}+x_2w^1_{12}+x_3w^1_{13}+x_4w^1_{14}+b_1^1 \\
h^0_2 = x_1w^1_{21}+x_2w^1_{22}+x_3w^1_{23}+x_4w^1_{24}+b_2^1 \\
h^0_3 = x_1w^1_{31}+x_2w^1_{32}+x_3w^1_{33}+x_4w^1_{34}+b_3^1 \\
h^0_4 = x_1w^1_{41}+x_2w^1_{42}+x_3w^1_{43}+x_4w^1_{44}+b_4^1 \\
h^0_5 = x_1w^1_{51}+x_2w^1_{52}+x_3w^1_{53}+x_4w^1_{54}+b_5^1 \\
\end{cases}
$$
$$
H=\begin{cases}
h_1=max\{0,h^0_1\} \\
h_2=max\{0,h^0_2\} \\
h_3=max\{0,h^0_3\} \\
h_4=max\{0,h^0_4\} \\
h_5=max\{0,h^0_5\} \\
\end{cases}
$$
è¾“å‡ºå±‚çš„è¾“å‡ºä¸º$\hat Y$
$$\hat {\mathbfit y}=\begin{bmatrix}\hat{y_1} & \hat{y_2} & \hat{y_3}\end{bmatrix}^T=Softmax(W^2H+b^2)$$
$$\mathbfit o=\begin{cases}
o_1 = h_1w^2_{11}+h_2w^2_{12}+h_3w^2_{13}+h_4w^2_{14}+h_5w^2_{15}+b^2_1 \\
o_2 = h_1w^2_{21}+h_2w^2_{22}+h_3w^2_{23}+h_4w^2_{24}+h_5w^2_{25}+b^2_2 \\
o_3 = h_1w^2_{32}+h_2w^2_{32}+h_3w^2_{33}+h_4w^2_{34}+h_5w^2_{35}+b^2_3 \\
\end{cases}$$
$$\begin{cases}
\hat {y_1} = \frac{\exp(o_1)}{\Sigma_i \exp(o_i)} \\
\hat {y_2} = \frac{\exp(o_2)}{\Sigma_i \exp(o_i)} \\
\hat {y_3} = \frac{\exp(o_3)}{\Sigma_i \exp(o_i)} \\
\end{cases}$$
æ ·æœ¬$\mathbfit x$æœ‰æ ‡ç­¾$\mathbfit y=\begin{bmatrix}y_1 & y_2 & y_3\end{bmatrix}$ï¼Œè¯´æ˜æ ·æœ¬è¢«åˆ†ä¸ºæ€»å…±3ä¸ªç±»ï¼Œä½¿ç”¨One-hotè¡¨ç¤ºæ–¹å¼ï¼Œè‹¥æ ·æœ¬å±äºç±»åˆ«2ï¼Œåˆ™$\mathbfit y=\begin{bmatrix}0&1&0\end{bmatrix}$ã€‚

## Safe Softmax

$$softmax(x_i)=\frac{e^{x_i}}{\Sigma_j^Ne^{x^j}}$$
Softmaxå‡½æ•°èµ·åˆ°äº†å½’ä¸€åŒ–çš„ä½œç”¨ï¼Œä½†æ˜¯å­˜åœ¨æ•°å€¼ç¨³å®šçš„é—®é¢˜ï¼Œå³æ•°å€¼è¿‡å¤§æ—¶æŒ‡æ•°å‡½æ•°ä¼šæº¢å‡ºã€‚Safe softmaxé€šè¿‡å‡å»è¾“å…¥å‘é‡ä¸­çš„æœ€å¤§å€¼è§£å†³è¿™ä¸€é—®é¢˜ã€‚
$$safe\ softmax(x_i)=\frac{e^{x_i-max(x)}}{\Sigma_j^Ne^{x_j-max(x)}}$$
