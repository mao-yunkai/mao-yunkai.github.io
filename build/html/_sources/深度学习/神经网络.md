# 神经网络
## 通用近似定理
通用近似定理（universal approximation theorem）(Hornik et al., 1989; Cybenko, 1989) 表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种 ‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。
## 基本模型
[torch.nn — PyTorch 2.5 documentation](https://pytorch.org/docs/stable/nn.html#linear-layers)
神经网络可以表示任意变换，因此，只要有数据和标签，就可以使用神经网络学习数据与标签之间的关系。神经网络层由两部分构成，第一部分是线性输出层，可以满足任意线性变换；第二部分是[[激活函数#激活函数]]，可以提供非线性变换能力。
单个神经元的结构为
![](attachments/SingleModel.png)

> 💡 **为什么$b$是一维而$W$是多维？**
> 假设$B\in \mathbb R^{d\times h}$，根据公式$H=XW+B$，那么可以得到$$\begin{align}
h_1&=w_1\times x_1+b_1+w_2\times x_2+b_2+w_3\times x_3+b_3+w_4\times x_4+b_4\\
&=w_1\times x_1+w_2\times x_2+w_3\times x_3+w_4\times x_4+(b_1+b_2+b_3+b_4)\\
&=w_1\times x_1+w_2\times x_2+w_3\times x_3+w_4\times x_4+b\\
&=xW+b
\end{align}$$因此$b$只需要一维即可。

以一个单隐藏层神经网络为例

![](attachments/NeuralNetwork.png)

设输入$X\in\mathbb R^{n\times d}$表示$n$个具有$d$维特征的样本，$H\in \mathbb R^{n\times h}$表示隐藏层的输出，$O\in \mathbb R^{n\times q}$表示输出层的输出。输入一个样本$x=[x_1, x_2, x_3, x_4]$，首先经过隐藏层，公式为$$H=\sigma(xW^1+b^1)$$其中$W^1\in \mathbb R^{d\times h}$，$b^1\in\mathbb R^{1\times h}$，$\sigma(x)$是激活函数，如[[激活函数#ReLU]]。
隐藏表示的特征再输入输出层，公式为$$O=HW^2+b^2$$其中$W^2\in \mathbb R^{h\times q}$，$b^2\in \mathbb R^{1\times q}$。
此时得到的输出$O$的取值范围是$\mathbb R$。在拟合任务中，我们已经得到了我们希望的输出；在分类任务中，我们希望输出层的输出可以表示预测出的类别分布概率。因此，我们需要用softmax函数处理输出结果。
$$\hat Y=softmax(O),其中,\hat {y_i}=\frac{e^i}{\Sigma_je^j}$$
在Pytorch中，实现上述模型的代码为
```Python
nn.Sequential(
	nn.Linear(in_features, hidden_features),
	nn.ReLU(),
	nn.Linear(hidden_features, out_features),
	nn.Softmax(dim=1)
)
```
in_features是输入层的神经网络节点个数，hidden_features是隐藏层的神经网络节点个数，out_features是输出层神经网络的节点个数。

> 💡**为什么Pytorch实现中通常不加入Softmax层？**
> Pytorch在nn.CrossEntropyLoss中隐式的实现了Softmax操作。如果使用的Loss函数不包括此功能，同时标签也是以One-hot格式组织，那么就需要自己实现Softmax操作。

> 💡**为什么不能用单层神经网络作为通用模型，而需要设计不同的模型架构？**
>万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的 MLP 一 定能够**表示**这个函数。然而，我们不能保证训练算法能够**学得**这个函数。即使 MLP 能够表示该函数，学习也可能因两个不同的原因而失败。首先，用于训练的优化算 法可能找不到用于期望函数的参数值。其次，训练算法可能由于过拟合而选择了错误的函数。前馈网络提供了表示函数的万能系统，在这种意义上，给定一个函数，深度前馈网络存在一个前馈网络能够近似该函数。不存在万能的过程既能够验证训练集上的特殊样本，又能够选择一个函数来扩展到训练集上没有的点。
>总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现， 并且可能无法正确地学习和泛化。在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。


线性代数的矩阵表示方式看起来不那么直观，将所有参数展开表示，我们来看一看数据在经过模型时发生了什么。基本模型的参数包括

$$
\begin{align}
\begin{bmatrix}W^1 & b^1\end{bmatrix}^T &= \begin{bmatrix}
w^1_{11} & w^1_{12} & w^1_{13} & w^1_{14} & w^1_{15} \\
w^1_{21} & w^1_{22} & w^1_{23} & w^1_{24} & w^1_{25} \\
w^1_{31} & w^1_{32} & w^1_{33} & w^1_{34} & w^1_{35} \\
w^1_{41} & w^1_{42} & w^1_{43} & w^1_{44} & w^1_{45} \\
b^1_1    & b^1_2    & b^1_3    & b^1_4    & b^1_5
\end{bmatrix} \\
\begin{bmatrix}W^2 & b^2\end{bmatrix}^T &= \begin{bmatrix}
w^2_{11} & w^2_{12} & w^2_{13} \\
w^2_{21} & w^2_{22} & w^2_{23} \\
w^2_{31} & w^2_{32} & w^2_{33} \\
w^2_{41} & w^2_{42} & w^2_{43} \\
w^2_{51} & w^2_{52} & w^2_{53} \\
b^2_1    & b^2_2    & b^2_3 
\end{bmatrix}
\end{align}
$$

输入一个样本$\mathbfit x=[x_1\ x_2\ x_3\ x_4]^T$，隐藏层输出为$H$

$$
H=\begin{bmatrix}h_1 & h_2 & h_3 & h_4 & h_5\end{bmatrix}^T=ReLU(W^1\mathbfit x+b^1)
$$
$$
H^0=\begin{cases}
h^0_1 = x_1w^1_{11}+x_2w^1_{12}+x_3w^1_{13}+x_4w^1_{14}+b_1^1 \\
h^0_2 = x_1w^1_{21}+x_2w^1_{22}+x_3w^1_{23}+x_4w^1_{24}+b_2^1 \\
h^0_3 = x_1w^1_{31}+x_2w^1_{32}+x_3w^1_{33}+x_4w^1_{34}+b_3^1 \\
h^0_4 = x_1w^1_{41}+x_2w^1_{42}+x_3w^1_{43}+x_4w^1_{44}+b_4^1 \\
h^0_5 = x_1w^1_{51}+x_2w^1_{52}+x_3w^1_{53}+x_4w^1_{54}+b_5^1 \\
\end{cases}
$$
$$
H=\begin{cases}
h_1=max\{0,h^0_1\} \\
h_2=max\{0,h^0_2\} \\
h_3=max\{0,h^0_3\} \\
h_4=max\{0,h^0_4\} \\
h_5=max\{0,h^0_5\} \\
\end{cases}
$$
输出层的输出为$\hat Y$
$$\hat {\mathbfit y}=\begin{bmatrix}\hat{y_1} & \hat{y_2} & \hat{y_3}\end{bmatrix}^T=Softmax(W^2H+b^2)$$
$$\mathbfit o=\begin{cases}
o_1 = h_1w^2_{11}+h_2w^2_{12}+h_3w^2_{13}+h_4w^2_{14}+h_5w^2_{15}+b^2_1 \\
o_2 = h_1w^2_{21}+h_2w^2_{22}+h_3w^2_{23}+h_4w^2_{24}+h_5w^2_{25}+b^2_2 \\
o_3 = h_1w^2_{32}+h_2w^2_{32}+h_3w^2_{33}+h_4w^2_{34}+h_5w^2_{35}+b^2_3 \\
\end{cases}$$
$$\begin{cases}
\hat {y_1} = \frac{\exp(o_1)}{\Sigma_i \exp(o_i)} \\
\hat {y_2} = \frac{\exp(o_2)}{\Sigma_i \exp(o_i)} \\
\hat {y_3} = \frac{\exp(o_3)}{\Sigma_i \exp(o_i)} \\
\end{cases}$$
样本$\mathbfit x$有标签$\mathbfit y=\begin{bmatrix}y_1 & y_2 & y_3\end{bmatrix}$，说明样本被分为总共3个类，使用One-hot表示方式，若样本属于类别2，则$\mathbfit y=\begin{bmatrix}0&1&0\end{bmatrix}$。

## Safe Softmax

$$softmax(x_i)=\frac{e^{x_i}}{\Sigma_j^Ne^{x^j}}$$
Softmax函数起到了归一化的作用，但是存在数值稳定的问题，即数值过大时指数函数会溢出。Safe softmax通过减去输入向量中的最大值解决这一问题。
$$safe\ softmax(x_i)=\frac{e^{x_i-max(x)}}{\Sigma_j^Ne^{x_j-max(x)}}$$
