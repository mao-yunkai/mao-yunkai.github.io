# 反向传播
[[神经网络#基本模型]]中定义的神经网络，使用[[损失函数#CrossEntropyLoss]]计算模型预测结果$\hat Y$与真实标签$Y$的损失
$$\begin{cases}
l_1=-y_1\log{\hat {y_1}} \\
l_2=-y_2\log{\hat {y_2}} \\
l_3=-y_3\log{\hat {y_3}} \\
\end{cases}$$
$$
J(\theta)=\frac{1}{3}(l_1+l_2+l_3)
$$
到此，我们得到单样本$x$和标签$y$经过深度前向传播网络得到的预测结果$\hat y$和标签$y$之间的损失值。反向传播需要解决的问题是，如何利用损失调整参数，使得下次推断的损失值变得更小。
## 梯度下降
> 💡**Jacobian矩阵和Hessian矩阵**
> Jacobian矩阵的定义是，如果我们有一个函数$\mathbfit f: \mathbb R^m\rightarrow\mathbb R^n$，$\mathbfit f$的Jacobian矩阵$\mathbfit J\in \mathbb R^{n\times m}$定义为$$J_{i,j}=\frac{\partial}{\partial_{x_j}}f(\mathbfit x)_i$$Hessian矩阵的定义是$$\mathbfit H(f)(\mathbfit x)_{i,j}=\frac{\partial^2}{\partial x_i\partial x_j}f(\mathbfit x)$$Hessian等价于梯度的Jacobian矩阵。

优化指的是改变$x$以最小化或最大化某个函数$f(x)$的任务。最优化问题通常指最小化函数。
[[分析学#梯度]]是在当前参数值下对目标函数每个参数进行求偏导得到的向量。以[[神经网络#基本模型]]为例，目标函数$J(\theta)$的梯度为
$$\Delta J(\theta)=(\frac{\partial J}{\partial w^1_{11}}, \frac{\partial J}{\partial w^1_{12}},\cdots,\frac{\partial J}{\partial b^2_3})$$
计算参数$w^1_{11}$的偏导数，使用链式法则
$$\frac{\partial J}{\partial w^1_{11}}=\frac{\partial J}{\partial \hat {\mathbfit y}}
\frac{\partial \hat {\mathbfit y}}{\partial {\mathbfit o}}
\frac{\partial \mathbfit o}{\partial \mathbfit h}
\frac{\partial \mathbfit h}{\partial \mathbfit h^0}
\frac{\partial \mathbfit h^0}{\partial w^1_{11}}
$$
设学习率为$\eta$，最终参数$w^1_{11}$的变化量为
$$w^1_{11}=w^1_{11}+\eta \frac{\partial J}{\partial w^1_{11}}$$
对交叉熵函数求导，$\frac{\partial J}{\partial \hat {\mathbfit y}}\in \mathbb R^{1\times3}$
$$\frac{\partial J}{\partial \hat {\mathbfit y}}=[-\Sigma \frac{y_1}{\hat y_1}\:-\Sigma \frac{y_2}{\hat y_2}\:-\Sigma \frac{y_3}{\hat y_3}]$$
对Softmax函数求导得到的Jacobian矩阵$\frac{\partial \hat {\mathbfit y}}{\partial \mathbfit o}\in \mathbb R^{3\times3}$，对Softmax Cross Entropy Loss求导得到的结果是$$\frac{\partial J}{\partial \mathbfit o}=\hat{\mathbfit y}-\mathbfit y$$

对输出层求导$\frac{\partial\mathbfit o}{\partial\mathbfit h}\in\mathbb R^{3\times5}$
$$\frac{\partial o_j}{\partial h_i}=w_{ji}^2$$
对激活函数求导$\frac{\partial \mathbfit h}{\partial \mathbfit {h_0}}\in\mathbb R^{5\times5}$
$$\frac{\partial h_j}{\partial h^0_i}=
\begin{cases}
1,h_i>0\\
0,h_i\leq0
\end{cases}
$$
对线性模型求导$\frac{\partial \mathbfit h^0}{\partial w_{11}^1}\in\mathbb R^{5\times1}$
$$\frac{\partial  \mathbfit h^0}{\partial w^1_{11}}=[x_1,0,0,0,0]$$
### 批量梯度下降
输入一个样本经过损失函数可以得到一个交叉熵，对$\mathbfit W$求导可以得到一个梯度。数据集$\mathbfit X\in \mathbb R^{n\times d}$中有$n$个样本，意味着可以计算出$n$个梯度，每个梯度是关于$\mathbfit W$的函数在当前$\mathbfit W$值下的$n$个方向向量，将这些向量累和，则能得到在样本集上的梯度估计。因为是向量，累和后的方向更准确，累和导致的模长变大可以使用学习率进行调整。
### 随机梯度下降
使用批量梯度下降，会计算出所有样本梯度后再更新参数。这样的缺点是计算量大。随机梯度下降则使用一个随机样本或一批样本的梯度作为更新的值，这样计算量小。
