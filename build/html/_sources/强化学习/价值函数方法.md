# Deep Q Learning
Q-Learning采用表格的形式储存动作价值函数，缺点是无法解决连续状态空间的问题。动作价值函数可以使用神经网络方法表示，通过训练神经网络拟合动作价值函数。若按这种思路处理，会存在两个问题：
1. Q-Learning在每个step处更新Q值，样本连续采样，不符合独立同分布，难以用来训练模型。
2. Q-Learning更新Q值时需要用到$S_{t+1}$的值，在更新网络后，$S_{t+1}$的值也发生了变化，因此会发生震荡，难以收敛。
针对以上两个问题，DQN分别提出了解决办法：
1. 使用经验回放缓存，存储N个新采样样本，每次更新从中随机采样。
2. 使用目标函数网络，固定$S_{t+1}$的神经网络模型，等待更新C次之后再同步价值网络和目标网络。

算法步骤：
- 用随机的网络参数$\omega$初始化网络$Q_{\omega}(s,a)$
- 复制相同的参数$\omega^-\leftarrow\omega$来初始化目标网络$Q_{\omega^-}$
- 初始化经验回放池$R$
- for 序列e = 1 $\rightarrow$ T do
    - 获取环境初始状态$s_1$
    - for 时间步t = 1 $\rightarrow$ T do
        - 根据当前网络$Q_{\omega}(s,a)$以贪婪策略选择动作$a_t$，获得回报$r_t$和下一个状态$s_{t+1}$。
        - 将$s_t,a_t,r_t,s_{t+1}$加入经验回放池$R$。
        - 若$R$中数据足够，从$R$中采样N个数据${(s_i,a_i,r_i,s_{i+1})}_{i=1,...,N}。
        - 对每个数据，用目标网络计算$$y_i=r_i+\gamma\max_{a'}Q_{\omega^-}(s_{i+1},a')$$
        - 最小化目标损失$$L= 1/N \sum_{i=1}^{N}(y_i-Q_{\omega}(s_i,a_i))^2$$，以此更新当前网络$Q_\omega$。
    - end for
- end for

## Reference
1. Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015). https://doi.org/10.1038/nature14236

